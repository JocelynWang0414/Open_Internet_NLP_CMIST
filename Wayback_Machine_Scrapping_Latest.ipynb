{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1vuqBs5VTCHr",
        "soa-OSwLm7Wl",
        "HG_kyOhP5XgN",
        "czH4wMrtb_qu"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chayka/Vechirka"
      ],
      "metadata": {
        "id": "1vuqBs5VTCHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import re\n",
        "\n",
        "def scrape_chayka_links():\n",
        "    \"\"\"\n",
        "    Scrape all links from the archived Chayka.org website that match the format\n",
        "    <a href=\"/node/XXXXX\" rel=\"bookmark\">some text</a>\n",
        "    Returns a list of dictionaries with link text and URL.\n",
        "    \"\"\"\n",
        "    # URL of the archived page\n",
        "    url = \"https://web.archive.org/web/20250202215959/https://www.chayka.org/\"\n",
        "\n",
        "    # Add a user agent to mimic a browser request\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Add a small delay to be respectful to the web.archive.org servers\n",
        "        time.sleep(1)\n",
        "\n",
        "        # Send GET request to the URL\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        # Raise an exception for bad status codes\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Parse HTML content\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find all <a> tags with rel=\"bookmark\" and href matching /node/XXXXX\n",
        "        bookmark_links = []\n",
        "\n",
        "        # Find all <a> tags with rel=\"bookmark\"\n",
        "        for link in soup.find_all('a', rel='bookmark'): #(chayk)\n",
        "            href = link.get('href', '')\n",
        "            # Check if the href matches the pattern \"/node/XXXXX\"\n",
        "\n",
        "            if \"node\" in href and not \".jpg\" in link.text and len(link.text)>0: #(chayk)\n",
        "                # Create full URL by combining base URL with relative path\n",
        "                full_url = f\"https://web.archive.org/web/20250202215959/https://www.chayka.org{href}\"\n",
        "                bookmark_links.append({\n",
        "                    \"text\": link.text.strip(),\n",
        "                    \"relative_url\": href,\n",
        "                    \"full_url\": full_url\n",
        "                })\n",
        "\n",
        "        return {\n",
        "            \"bookmark_links\": bookmark_links,\n",
        "            \"count\": len(bookmark_links),\n",
        "            \"source_url\": url\n",
        "        }\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return {\"error\": f\"Request error: {str(e)}\"}\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"General error: {str(e)}\"}\n"
      ],
      "metadata": {
        "id": "ST1C1XlMTFlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(f\"Starting to scrape bookmark links from Chayka...\")\n",
        "    result = scrape_chayka_links()\n",
        "\n",
        "    if \"error\" in result:\n",
        "        print(result[\"error\"])\n",
        "    else:\n",
        "        print(f\"Found {result['count']} bookmark links on the page.\\n\")\n",
        "\n",
        "        print(\"Bookmark Links:\")\n",
        "        for i, link in enumerate(result['bookmark_links'], 1):\n",
        "            print(f\"{i}. Text: {link['text']}\")\n",
        "            print(f\"   URL: {link['relative_url']}\")\n",
        "            print(f\"   Full URL: {link['full_url']}\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "    print(\"Scraping completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycMS2KZ_TGzE",
        "outputId": "df21f06c-35c2-4489-cb96-ad0422b05f7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to scrape bookmark links from Chayka...\n",
            "Found 17 bookmark links on the page.\n",
            "\n",
            "Bookmark Links:\n",
            "1. Text: Православная церковь в мире. Zoom-встреча с отцом Джованни Гуайтой 9 февраля\n",
            "   URL: /web/20250202215959/https://www.chayka.org/node/15874\n",
            "   Full URL: https://web.archive.org/web/20250202215959/https://www.chayka.org/web/20250202215959/https://www.chayka.org/node/15874\n",
            "--------------------------------------------------------------------------------\n",
            "2. Text: Согласись, Маня! Доводы мужа, который не хочет встать с дивана\n",
            "   URL: /web/20250202215959/https://www.chayka.org/node/15912\n",
            "   Full URL: https://web.archive.org/web/20250202215959/https://www.chayka.org/web/20250202215959/https://www.chayka.org/node/15912\n",
            "--------------------------------------------------------------------------------\n",
            "3. Text: На водосборах Ойкумены. В продолжение темы\n",
            "   URL: /web/20250202215959/https://www.chayka.org/node/15911\n",
            "   Full URL: https://web.archive.org/web/20250202215959/https://www.chayka.org/web/20250202215959/https://www.chayka.org/node/15911\n",
            "--------------------------------------------------------------------------------\n",
            "4. Text: Живет в душе и памяти\n",
            "   URL: /web/20250202215959/https://www.chayka.org/node/15910\n",
            "   Full URL: https://web.archive.org/web/20250202215959/https://www.chayka.org/web/20250202215959/https://www.chayka.org/node/15910\n",
            "--------------------------------------------------------------------------------\n",
            "5. Text: Тиран и Принцесса\n",
            "   URL: /web/20250202215959/https://www.chayka.org/node/15907\n",
            "   Full URL: https://web.archive.org/web/20250202215959/https://www.chayka.org/web/20250202215959/https://www.chayka.org/node/15907\n",
            "--------------------------------------------------------------------------------\n",
            "6. Text: Ржавою копейкой\n",
            "   URL: /web/20250202215959/https://www.chayka.org/node/15906\n",
            "   Full URL: https://web.archive.org/web/20250202215959/https://www.chayka.org/web/20250202215959/https://www.chayka.org/node/15906\n",
            "--------------------------------------------------------------------------------\n",
            "7. Text: Трагедия вблизи аэропорта имени Рональда Рейгана\n",
            "   URL: /web/20250202215959/https://www.chayka.org/node/15905\n",
            "   Full URL: https://web.archive.org/web/20250202215959/https://www.chayka.org/web/20250202215959/https://www.chayka.org/node/15905\n",
            "--------------------------------------------------------------------------------\n",
            "8. Text: Русский еврей в фашистской Италии: Филипп Вейцман\n",
            "   URL: /web/20250202215959/https://www.chayka.org/node/15904\n",
            "   Full URL: https://web.archive.org/web/20250202215959/https://www.chayka.org/web/20250202215959/https://www.chayka.org/node/15904\n",
            "--------------------------------------------------------------------------------\n",
            "9. Text: Два рассказа. Из цикла «Опыт прощания»\n",
            "   URL: /web/20250202215959/https://www.chayka.org/node/15903\n",
            "   Full URL: https://web.archive.org/web/20250202215959/https://www.chayka.org/web/20250202215959/https://www.chayka.org/node/15903\n",
            "--------------------------------------------------------------------------------\n",
            "10. Text: Операция «Крылья свободы»\n",
            "   URL: /web/20250202215959/https://www.chayka.org/node/15902\n",
            "   Full URL: https://web.archive.org/web/20250202215959/https://www.chayka.org/web/20250202215959/https://www.chayka.org/node/15902\n",
            "--------------------------------------------------------------------------------\n",
            "11. Text: К столетию со дня рождения художника Вячеслава Иляхинского\n",
            "   URL: /web/20250202215959/https://www.chayka.org/node/15901\n",
            "   Full URL: https://web.archive.org/web/20250202215959/https://www.chayka.org/web/20250202215959/https://www.chayka.org/node/15901\n",
            "--------------------------------------------------------------------------------\n",
            "12. Text: Четверостишия\n",
            "   URL: /web/20250202215959/https://www.chayka.org/node/15900\n",
            "   Full URL: https://web.archive.org/web/20250202215959/https://www.chayka.org/web/20250202215959/https://www.chayka.org/node/15900\n",
            "--------------------------------------------------------------------------------\n",
            "13. Text: Это еще не конец. О войне Израиля с Хамасом\n",
            "   URL: /web/20250202215959/https://www.chayka.org/node/15899\n",
            "   Full URL: https://web.archive.org/web/20250202215959/https://www.chayka.org/web/20250202215959/https://www.chayka.org/node/15899\n",
            "--------------------------------------------------------------------------------\n",
            "14. Text: Америка и Канада: история, политика и возможность объединения\n",
            "   URL: /web/20250202215959/https://www.chayka.org/node/15898\n",
            "   Full URL: https://web.archive.org/web/20250202215959/https://www.chayka.org/web/20250202215959/https://www.chayka.org/node/15898\n",
            "--------------------------------------------------------------------------------\n",
            "15. Text: Три маленьких рассказа\n",
            "   URL: /web/20250202215959/https://www.chayka.org/node/15896\n",
            "   Full URL: https://web.archive.org/web/20250202215959/https://www.chayka.org/web/20250202215959/https://www.chayka.org/node/15896\n",
            "--------------------------------------------------------------------------------\n",
            "16. Text: Памяти Геннадия Крочика\n",
            "   URL: /web/20250202215959/https://www.chayka.org/node/6015\n",
            "   Full URL: https://web.archive.org/web/20250202215959/https://www.chayka.org/web/20250202215959/https://www.chayka.org/node/6015\n",
            "--------------------------------------------------------------------------------\n",
            "17. Text: Согласись, Маня! Доводы мужа, который не хочет встать с дивана\n",
            "   URL: /web/20250202215959/https://www.chayka.org/node/15912\n",
            "   Full URL: https://web.archive.org/web/20250202215959/https://www.chayka.org/web/20250202215959/https://www.chayka.org/node/15912\n",
            "--------------------------------------------------------------------------------\n",
            "Scraping completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vechirka (partial)"
      ],
      "metadata": {
        "id": "soa-OSwLm7Wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import re\n",
        "\n",
        "def scrape_vechirka_links():\n",
        "    \"\"\"\n",
        "    Scrape all links from the archived Chayka.org website that match the format\n",
        "    <a href=\"/node/XXXXX\" rel=\"bookmark\">some text</a>\n",
        "    Returns a list of dictionaries with link text and URL.\n",
        "    \"\"\"\n",
        "    # URL of the archived page\n",
        "    url = 'https://web.archive.org/web/20220203004906/http://vechirka.pl.ua/'\n",
        "\n",
        "    # Add a user agent to mimic a browser request\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Add a small delay to be respectful to the web.archive.org servers\n",
        "        time.sleep(1)\n",
        "\n",
        "        # Send GET request to the URL\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        # Raise an exception for bad status codes\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Parse HTML content\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find all <a> tags with rel=\"bookmark\" and href matching /node/XXXXX\n",
        "        bookmark_links = []\n",
        "\n",
        "        # Find all <a> tags with rel=\"bookmark\"\n",
        "        #for link in soup.find_all('a', rel='bookmark'): (chayk)\n",
        "        for link in soup.find_all('a'):\n",
        "            href = link.get('href', '')\n",
        "            # Check if the href matches the pattern \"/node/XXXXX\"\n",
        "\n",
        "            # if \"node\" in href and not \".jpg\" in link.text and len(link.text)>0: (chayk)\n",
        "            if \"ukrayina\" in href and len(link.text)>0 and href[-1] in \"0123456789\": #vechirka\n",
        "                # Create full URL by combining base URL with relative path\n",
        "                full_url = f\"https://web.archive.org/web/20250202215959/https://www.chayka.org{href}\"\n",
        "                bookmark_links.append({\n",
        "                    \"text\": link.text.strip(),\n",
        "                    \"relative_url\": href,\n",
        "                    \"full_url\": full_url\n",
        "                })\n",
        "\n",
        "        return {\n",
        "            \"bookmark_links\": bookmark_links,\n",
        "            \"count\": len(bookmark_links),\n",
        "            \"source_url\": url\n",
        "        }\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return {\"error\": f\"Request error: {str(e)}\"}\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"General error: {str(e)}\"}\n"
      ],
      "metadata": {
        "id": "Q4X3cRpem6id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(f\"Starting to scrape bookmark links from vechirka.pl.ua...\")\n",
        "    result = scrape_chayka_links()\n",
        "\n",
        "\n",
        "    if \"error\" in result:\n",
        "        print(result[\"error\"])\n",
        "    else:\n",
        "        print(f\"Found {result['count']} bookmark links on the page.\\n\")\n",
        "\n",
        "        print(\"Bookmark Links:\")\n",
        "        for i, link in enumerate(result['bookmark_links'], 1):\n",
        "            print(f\"{i}. Text: {link['text']}\")\n",
        "            print(f\"   URL: {link['relative_url']}\")\n",
        "            print(f\"   Full URL: {link['full_url']}\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "    print(\"Scraping completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZswxivDBnCd-",
        "outputId": "407b73aa-109a-4f5e-fb1a-e692ee954f33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to scrape bookmark links from vechirka.pl.ua...\n",
            "Found 9 bookmark links on the page.\n",
            "\n",
            "Bookmark Links:\n",
            "1. Text: «Я, звичайно, не хочу нікого нaлякa...\n",
            "   URL: /web/20220203004906/http://vechirka.pl.ua/ukrayina/1614239425\n",
            "   Full URL: https://web.archive.org/web/20250202215959/https://www.chayka.org/web/20220203004906/http://vechirka.pl.ua/ukrayina/1614239425\n",
            "--------------------------------------------------------------------------------\n",
            "2. Text: Чому маємо безіменних загиблих захи...\n",
            "   URL: /web/20220203004906/http://vechirka.pl.ua/ukrayina/1576827290\n",
            "   Full URL: https://web.archive.org/web/20250202215959/https://www.chayka.org/web/20220203004906/http://vechirka.pl.ua/ukrayina/1576827290\n",
            "--------------------------------------------------------------------------------\n",
            "3. Text: Росіяни вважають США та Україну най...\n",
            "   URL: /web/20220203004906/http://vechirka.pl.ua/ukrayina/1573717439\n",
            "   Full URL: https://web.archive.org/web/20250202215959/https://www.chayka.org/web/20220203004906/http://vechirka.pl.ua/ukrayina/1573717439\n",
            "--------------------------------------------------------------------------------\n",
            "4. Text: За фальсифікацію ліків в Україні за...\n",
            "   URL: /web/20220203004906/http://vechirka.pl.ua/ukrayina/1573116812\n",
            "   Full URL: https://web.archive.org/web/20250202215959/https://www.chayka.org/web/20220203004906/http://vechirka.pl.ua/ukrayina/1573116812\n",
            "--------------------------------------------------------------------------------\n",
            "5. Text: Душа войны\n",
            "   URL: /web/20220203004906/http://vechirka.pl.ua/ukrayina/1643269051\n",
            "   Full URL: https://web.archive.org/web/20250202215959/https://www.chayka.org/web/20220203004906/http://vechirka.pl.ua/ukrayina/1643269051\n",
            "--------------------------------------------------------------------------------\n",
            "6. Text: читати статтю\n",
            "   URL: /web/20220203004906/http://vechirka.pl.ua/ukrayina/1643269051\n",
            "   Full URL: https://web.archive.org/web/20250202215959/https://www.chayka.org/web/20220203004906/http://vechirka.pl.ua/ukrayina/1643269051\n",
            "--------------------------------------------------------------------------------\n",
            "7. Text: Одкровення, або З думою про хліб насущний\n",
            "   URL: /web/20220203004906/http://vechirka.pl.ua/ukrayina/1642057779\n",
            "   Full URL: https://web.archive.org/web/20250202215959/https://www.chayka.org/web/20220203004906/http://vechirka.pl.ua/ukrayina/1642057779\n",
            "--------------------------------------------------------------------------------\n",
            "8. Text: читати статтю\n",
            "   URL: /web/20220203004906/http://vechirka.pl.ua/ukrayina/1642057779\n",
            "   Full URL: https://web.archive.org/web/20250202215959/https://www.chayka.org/web/20220203004906/http://vechirka.pl.ua/ukrayina/1642057779\n",
            "--------------------------------------------------------------------------------\n",
            "9. Text: Душа войны\n",
            "   URL: /web/20220203004906/http://vechirka.pl.ua/ukrayina/1643269051\n",
            "   Full URL: https://web.archive.org/web/20250202215959/https://www.chayka.org/web/20220203004906/http://vechirka.pl.ua/ukrayina/1643269051\n",
            "--------------------------------------------------------------------------------\n",
            "Scraping completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Vechirka newspaper scrapping pipeline (updated on March 28)"
      ],
      "metadata": {
        "id": "hVSL2pXNpqXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Get the date & time from Wayback machine\n",
        "## 2. Know the max_article to scrap on that page\n",
        "## 3. Know the column of interest (policy analytics/on the front line)\n",
        "## 4. Obtain relative url (without Wayback Machine archive marks, which can make the content inaccessble)\n",
        "## 5. Get content using the \"extract content pipeline\" function\n",
        "## 6. Get a json file with all articles**"
      ],
      "metadata": {
        "id": "EeSODazrT1RV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import re\n",
        "import json"
      ],
      "metadata": {
        "id": "lIX0bQ7i6VcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vechirka_links(base_url):\n",
        "    \"\"\"\n",
        "    Scrape all relevant links from the archived vechirka.pl.ua website.\n",
        "    Returns a list of dictionaries with link text and URL.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        time.sleep(1)  # Be polite to the server\n",
        "        response = requests.get(base_url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        bookmark_links = []\n",
        "        for link in soup.find_all('a'):\n",
        "            href = link.get('href', '')\n",
        "            link_text = link.text.strip()\n",
        "\n",
        "            # Check if the href matches the pattern we're looking for (ends with a digit and contains \"ukrayina\")\n",
        "            if \"ukrayina\" in href and len(link_text) > 0 and href[-1] in \"0123456789\":\n",
        "                # Extract archive timestamp from base_url\n",
        "                archive_timestamp = re.search(r'web/(\\d+)/', base_url).group(1) if re.search(r'web/(\\d+)/', base_url) else \"20220203004906\"\n",
        "\n",
        "                # Create proper archive URL\n",
        "                full_url = f\"https://web.archive.org/web/{archive_timestamp}/http://vechirka.pl.ua{href}\"\n",
        "                bookmark_links.append({\n",
        "                    \"text\": link_text,\n",
        "                    \"relative_url\": href,\n",
        "                    \"full_url\": full_url,\n",
        "                    \"article_content\": None  # Will be filled in later\n",
        "                })\n",
        "\n",
        "        return bookmark_links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting vechirka links: {str(e)}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "tM3XqAX86X5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deep-translator\n",
        "from deep_translator import GoogleTranslator\n",
        "def deep_translate_to_english(ukrainian_text):\n",
        "    print('deep translating now...')\n",
        "    output = ''\n",
        "    print('length of the text: ', len(ukrainian_text))\n",
        "    if len(ukrainian_text) < 500:\n",
        "      return GoogleTranslator(source='uk', target='en').translate(ukrainian_text)\n",
        "    else:\n",
        "      i = 0\n",
        "      while i < len(ukrainian_text):\n",
        "        if i <= len(ukrainian_text) - 500:\n",
        "          output += GoogleTranslator(source='uk', target='en').translate(ukrainian_text[i:i+500])\n",
        "        else:\n",
        "          output += GoogleTranslator(source='uk', target='en').translate(ukrainian_text[i:])\n",
        "        i += 500\n",
        "    return output"
      ],
      "metadata": {
        "id": "cu5qBStVRh2B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63cc1d61-9514-479a-ff15-71d21976e2fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (4.13.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.1.31)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_article_content_pipepline(article_url):\n",
        "    \"\"\"\n",
        "    Visit an article page and extract the content.\n",
        "    For vechirka.pl.ua, we'll look for the main article content.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        time.sleep(2)  # Longer delay for article pages to be extra polite\n",
        "        response = requests.get(article_url, headers=headers)\n",
        "\n",
        "        response.raise_for_status()\n",
        "\n",
        "        if response.status_code != 200: print('Webscrapping response OK.')\n",
        "\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # First try to find content in a div with class field-item even (like in Chayka)\n",
        "        article_div = soup.find_all('div', class_='field-item even')\n",
        "\n",
        "\n",
        "\n",
        "        paragraphs = []\n",
        "        # for x in article_div:\n",
        "        #     print(x)\n",
        "        for p in article_div:\n",
        "        # for p in article_div.find_all('p'):\n",
        "            paragraphs.append(p.get_text(strip=False))\n",
        "\n",
        "\n",
        "        # Join paragraphs with double newlines to preserve structure\n",
        "        content = \"\\n\\n\".join(paragraphs)\n",
        "\n",
        "        content = deep_translate_to_english(content)\n",
        "\n",
        "        return content\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting article: {str(e)}\""
      ],
      "metadata": {
        "id": "XCLUsHrIPvQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def extract_article_content(article_url):\n",
        "#     \"\"\"\n",
        "#     Visit an article page and extract the content.\n",
        "#     For vechirka.pl.ua, we'll look for the main article content.\n",
        "#     \"\"\"\n",
        "#     headers = {\n",
        "#         \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "#     }\n",
        "\n",
        "#     try:\n",
        "#         time.sleep(2)  # Longer delay for article pages to be extra polite\n",
        "#         response = requests.get(article_url, headers=headers)\n",
        "#         response.raise_for_status()\n",
        "\n",
        "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "#         # First try to find content in a div with class field-item even (like in Chayka)\n",
        "#         article_div = soup.find_all('div', class_='field-item even')\n",
        "\n",
        "#         #article_div = soup.find('p')\n",
        "\n",
        "\n",
        "#         paragraphs = []\n",
        "#         for p in article_div:\n",
        "#         # for p in article_div.find_all('p'):\n",
        "#             paragraphs.append(p.get_text(strip=True))\n",
        "\n",
        "#         # Join paragraphs with double newlines to preserve structure\n",
        "#         content = \"\\n\\n\".join(paragraphs)\n",
        "\n",
        "#         # If no paragraphs were found, get all text from the div\n",
        "#         if not content:\n",
        "#             content = article_div.get_text(strip=True)\n",
        "\n",
        "#         return content\n",
        "\n",
        "#     except Exception as e:\n",
        "#         return f\"Error extracting article: {str(e)}\"\n",
        "\n",
        "\n",
        "def scrape_vechirka_articles(base_url, max_articles=5):\n",
        "    \"\"\"\n",
        "    Main function to scrape vechirka.pl.ua articles.\n",
        "    Limits the number of articles to scrape to avoid overloading the server.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"1. Finding relevant links on the main page...\")\n",
        "    vechirka_links = get_vechirka_links(base_url)\n",
        "\n",
        "    if not vechirka_links:\n",
        "        return {\"error\": \"No relevant links found\"}\n",
        "\n",
        "    # Limit the number of articles to scrape\n",
        "    articles_to_scrape = vechirka_links[:max_articles]\n",
        "\n",
        "    urls = []\n",
        "    for a in articles_to_scrape:\n",
        "      url = '/'.join(a['relative_url'].split('/')[-5:])\n",
        "      urls.append(url)\n",
        "\n",
        "    print(f\"2. Found {len(vechirka_links)} links. Will scrape {len(urls)} articles...\")\n",
        "\n",
        "    # Visit each article page and extract content\n",
        "    # for i, article in enumerate(articles_to_scrape, 1):\n",
        "    #     print(f\"   Scraping article {i}/{len(articles_to_scrape)}: {article['text'][:40]}...\")\n",
        "    #     #article_content = extract_article_content(article['full_url'])\n",
        "\n",
        "    #     #url = \"/\".join(article['relative_url'].split(\"/\")[3:])\n",
        "    #     url = base_url[:-1] + article['relative_url']\n",
        "\n",
        "    #     article_content = extract_article_content_pipeline(url)\n",
        "    #     article['article_content'] = article_content\n",
        "\n",
        "    for i in range(len(urls)):\n",
        "      print(f\"   Scraping article {i+1}/{len(urls)}: {urls[i][:40]}...\")\n",
        "      article_content = extract_article_content_pipepline(urls[i])\n",
        "      articles_to_scrape[i]['article_content'] = article_content\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"total_links_found\": len(vechirka_links),\n",
        "        \"articles_scraped\": len(urls),\n",
        "        \"articles\": articles_to_scrape\n",
        "    }"
      ],
      "metadata": {
        "id": "-NtbexLvppQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to json\n",
        "# def save_to_file(data, filename=\"Vechirka Articles 20230131051233 [English].json\"):\n",
        "#     \"\"\"Save the scraped data to a JSON file\"\"\"\n",
        "#     with open(filename, 'w', encoding='utf-8') as f:\n",
        "#         json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "#     print(f\"Data saved to {filename}\")\n",
        "\n",
        "# New: save to csv\n",
        "import csv\n",
        "def save_to_file(data, filename=None):\n",
        "    \"\"\"\n",
        "    Save the scraped data to a CSV file\n",
        "\n",
        "    Args:\n",
        "    data (list): List of dictionaries containing article information\n",
        "    filename (str, optional): Name of the output CSV file.\n",
        "                               If not provided, a default name will be generated.\n",
        "    \"\"\"\n",
        "    # Generate default filename if not provided\n",
        "    if filename is None:\n",
        "        from datetime import datetime\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "        filename = f\"Vechirka Articles {timestamp} [English].csv\"\n",
        "\n",
        "    # Ensure the filename has .csv extension\n",
        "    if not filename.lower().endswith('.csv'):\n",
        "        filename += '.csv'\n",
        "\n",
        "\n",
        "    # Determine the columns (assuming all dictionaries have the same keys)\n",
        "    if not data:\n",
        "        print(\"No data to save.\")\n",
        "        return\n",
        "\n",
        "    data['Title'] = data['articles'][0]['text']\n",
        "    data['Content'] = data['articles'][0]['article_content']\n",
        "    data['URL'] = data['articles'][0]['relative_url']\n",
        "    del data['articles']\n",
        "\n",
        "    fieldnames = list(data.keys())\n",
        "\n",
        "    print(list(data.keys()))\n",
        "    # Write to CSV\n",
        "    try:\n",
        "        with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=list(data.keys()))\n",
        "\n",
        "            # Write header\n",
        "            writer.writeheader()\n",
        "\n",
        "            # Write data rows\n",
        "            writer.writerows(data)\n",
        "\n",
        "        print(f\"Data saved to {filename}\")\n",
        "        print(f\"Total articles saved: {len(data)}\")\n",
        "\n",
        "    except IOError as e:\n",
        "        print(f\"Error saving file: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "id": "DA76nB4n6dlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"Starting vechirka.pl.ua article scraper...\")\n",
        "\n",
        "    # Limit to 5 articles by default to be gentle on the Internet Archive\n",
        "    # base_url = 'http://vechirka.pl.ua/'\n",
        "    max_articles = 1\n",
        "    base_url = 'https://web.archive.org/web/20230131051233/http://vechirka.pl.ua/ukrayina/na-liniyi-frontu'\n",
        "    result = scrape_vechirka_articles(base_url, max_articles)\n",
        "\n",
        "    if \"error\" in result:\n",
        "        print(f\"Error: {result['error']}\")\n",
        "    else:\n",
        "        print(f\"\\nSuccessfully scraped {result['articles_scraped']} articles out of {result['total_links_found']} links found.\")\n",
        "\n",
        "        # Display sample of the first article\n",
        "        if result['articles']:\n",
        "            first_article = result['articles'][0]\n",
        "            print(\"\\nSample of first article:\")\n",
        "            print(f\"Title: {first_article['text']}\")\n",
        "            print(f\"URL: {first_article['full_url']}\")\n",
        "            # Show first 200 characters of content\n",
        "            content_preview = first_article['article_content'][:200] + \"...\" if len(first_article['article_content']) > 200 else first_article['article_content']\n",
        "            print(f\"Content: {content_preview}\")\n",
        "\n",
        "        # Save all data to a file\n",
        "        save_to_file(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZXoV19W5eD5",
        "outputId": "165ae6a4-9fcc-4fe5-f992-fc06de6facdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting vechirka.pl.ua article scraper...\n",
            "1. Finding relevant links on the main page...\n",
            "2. Found 66 links. Will scrape 1 articles...\n",
            "   Scraping article 1/1: http://vechirka.pl.ua/ukrayina/164517005...\n",
            "deep translating now...\n",
            "length of the text:  679\n",
            "\n",
            "Successfully scraped 1 articles out of 66 links found.\n",
            "\n",
            "Sample of first article:\n",
            "Title: Термін, за який можна витратити «ко...\n",
            "URL: https://web.archive.org/web/20230131051233/http://vechirka.pl.ua/web/20230131051233/http://vechirka.pl.ua/ukrayina/1645170055\n",
            "Content: The term of use of 1,000 UAH \"support\" is 9 months instead of 4. This was reported on the Page \"Action\" on Facebook.\n",
            "If you have a \"green\" CVID certificate in \"Action\" and have not yet applied for pay...\n",
            "['total_links_found', 'articles_scraped', 'Title', 'Content', 'URL']\n",
            "Unexpected error occurred: 'str' object has no attribute 'keys'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract one article"
      ],
      "metadata": {
        "id": "HG_kyOhP5XgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deep-translator\n",
        "from deep_translator import GoogleTranslator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj8mW4Gtbp_F",
        "outputId": "814714c2-abb7-4148-9c39-a34960e41b11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (4.13.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.1.31)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import re\n",
        "import json"
      ],
      "metadata": {
        "id": "KYLF99PgF9R_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_article_content_pipepline(article_url):\n",
        "    \"\"\"\n",
        "    Visit an article page and extract the content.\n",
        "    For vechirka.pl.ua, we'll look for the main article content.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        time.sleep(2)  # Longer delay for article pages to be extra polite\n",
        "        response = requests.get(article_url, headers=headers)\n",
        "\n",
        "        response.raise_for_status()\n",
        "\n",
        "        print('here', response.status_code)\n",
        "\n",
        "        print(response.status_code != 200)\n",
        "\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # First try to find content in a div with class field-item even (like in Chayka)\n",
        "        article_div = soup.find_all('div', class_='field-item even')\n",
        "\n",
        "\n",
        "\n",
        "        paragraphs = []\n",
        "        # for x in article_div:\n",
        "        #     print(x)\n",
        "        for p in article_div:\n",
        "        # for p in article_div.find_all('p'):\n",
        "            paragraphs.append(p.get_text(strip=False))\n",
        "\n",
        "\n",
        "        # Join paragraphs with double newlines to preserve structure\n",
        "        content = \"\\n\\n\".join(paragraphs)\n",
        "\n",
        "        # If no paragraphs were found, get all text from the div\n",
        "        # if not content:\n",
        "        #     content = article_div.get_text(strip=True)\n",
        "        return content\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting article: {str(e)}\"\n",
        "\n",
        "full_url = 'http://vechirka.pl.ua/ukrayina/1618471692' # this works\n",
        "\n",
        "full_url = 'http://vechirka.pl.ua/ukrayina/1617267033'\n",
        "\n",
        "uk_output = extract_article_content_pipepline(full_url)\n",
        "\n",
        "# def translate_to_english(ukrainian_text):\n",
        "#     translator = Translator()\n",
        "#     ukrainian_text_list = ukrainian_text.split(' ')[:]\n",
        "#     english_translation = ''\n",
        "#     print(len(ukrainian_text_list))\n",
        "#     for i in range(len(ukrainian_text_list)):\n",
        "#         word = ukrainian_text_list[i].strip()\n",
        "#         # translated_word = translator.translate(word, src='uk', dest='en').text\n",
        "#         translated_word = GoogleTranslator(source='uk', target='en').translate(word)\n",
        "#         english_translation += str(translated_word) + ' '\n",
        "#     print(english_translation, len(english_translation))\n",
        "#     return english_translation\n",
        "\n",
        "def deep_translate_to_english(ukrainian_text):\n",
        "    output = ''\n",
        "    if len(ukrainian_text) < 500:\n",
        "      return GoogleTranslator(source='uk', target='en').translate(ukrainian_text)\n",
        "    else:\n",
        "      i = 0\n",
        "      while i < len(ukrainian_text):\n",
        "        if i <= len(ukrainian_text) - 500:\n",
        "          output += GoogleTranslator(source='uk', target='en').translate(ukrainian_text[i:i+500])\n",
        "        else:\n",
        "          output += GoogleTranslator(source='uk', target='en').translate(ukrainian_text[i:])\n",
        "        i += 500\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "content = deep_translate_to_english(uk_output)\n",
        "\n",
        "def save_to_file(data, filename):\n",
        "    \"\"\"Save the scraped data to a TXT file\"\"\"\n",
        "    filename = filename+'[english].txt'\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        # If 'data' is a list of strings, write each string on a new line\n",
        "        if isinstance(data, list):\n",
        "            for line in data:\n",
        "                f.write(f\"{line}\\n\")\n",
        "        # If 'data' is a single string, write it directly\n",
        "        elif isinstance(data, str):\n",
        "            f.write(data)\n",
        "        else:\n",
        "            raise ValueError(\"Data must be a string or a list of strings\")\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "save_to_file(content, full_url.split(\"/\")[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLRcxNUCwPqR",
        "outputId": "89ff9a12-75fc-4793-aecd-e6d8b54bf035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here 200\n",
            "False\n",
            "Data saved to 1617267033[english].txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **\"On the Frontline\"** Scrapping"
      ],
      "metadata": {
        "id": "czH4wMrtb_qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==4.0.0-rc1\n",
        "from googletrans import Translator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuOzi_ceeFQS",
        "outputId": "0f52be12-8265-437c-8363-77d35269309c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.31)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17396 sha256=b52eda397e3ec0cee3d7ce4da0fe70882d2c8cbc16862e564ffc1ffd64fbf93f\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/17/6f/66a045ea3d168826074691b4b787b8f324d3f646d755443fda\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.2.0\n",
            "    Uninstalling h2-4.2.0:\n",
            "      Successfully uninstalled h2-4.2.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.7\n",
            "    Uninstalling httpcore-1.0.7:\n",
            "      Successfully uninstalled httpcore-1.0.7\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langsmith 0.3.28 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.72.0 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "google-genai 1.10.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry"
      ],
      "metadata": {
        "id": "fzHo9FelFKna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AIyfPh4nYHNQ",
        "outputId": "40672132-0b51-4b30-9cf4-f5ea09e0b57c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.30.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.1.31)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.9.0)\n",
            "Downloading selenium-4.30.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.29.0-py3-none-any.whl (492 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.30.0 trio-0.29.0 trio-websocket-0.12.2 wsproto-1.2.0\n",
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,892 kB]\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,381 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,239 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [47.7 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,737 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,045 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,684 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,538 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,049 kB]\n",
            "Get:21 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,784 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Fetched 29.9 MB in 4s (8,300 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 snapd squashfs-tools systemd-hwe-hwdb udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 snapd squashfs-tools systemd-hwe-hwdb\n",
            "  udev\n",
            "0 upgraded, 8 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 30.2 MB of archives.\n",
            "After this operation, 123 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.12 [1,557 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.67.1+22.04 [27.8 MB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Fetched 30.2 MB in 1s (41.1 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 126209 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Selecting previously unselected package udev.\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.67.1+22.04_amd64.deb ...\n",
            "Unpacking snapd (2.67.1+22.04) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.12) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.67.1+22.04) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 126638 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.12) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install webdriver_manager"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iu2MwH6KYb_J",
        "outputId": "cfc3b846-cb96-42e7-bdff-7f8218e9b179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting webdriver_manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from webdriver_manager) (2.32.3)\n",
            "Collecting python-dotenv (from webdriver_manager)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from webdriver_manager) (24.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver_manager) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver_manager) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver_manager) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver_manager) (2025.1.31)\n",
            "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, webdriver_manager\n",
            "Successfully installed python-dotenv-1.1.0 webdriver_manager-4.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selectolax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ja2bdTazY1sw",
        "outputId": "168a1ae6-cf8c-4b54-902d-35e309b1ff33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selectolax\n",
            "  Downloading selectolax-0.3.28-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.1 kB)\n",
            "Downloading selectolax-0.3.28-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: selectolax\n",
            "Successfully installed selectolax-0.3.28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_to_english(ukrainian_text):\n",
        "    translator = Translator()\n",
        "    english_translation = translator.translate(ukrainian_text, dest='en').text\n",
        "    return english_translation\n",
        "\n",
        "sample_link_frontline_news = [] # obtain from Wayback machine without opening the page on WBM\n",
        "sample_link_frontline_news.append('https://web.archive.org/web/20211118031812/http://vechirka.pl.ua/')\n",
        "sample_link_frontline_news.append('https://web.archive.org/web/20220916000436/http://vechirka.pl.ua/')\n",
        "sample_link_frontline_news.append('https://web.archive.org/web/20250221001907/http://vechirka.pl.ua/')\n",
        "sample_link_frontline_news.append('https://web.archive.org/web/20230309045149/http://vechirka.pl.ua/')\n",
        "\n",
        "\n",
        "section_key_word = {}\n",
        "section_key_word['policy - analytics'] = 'polityka/analityka'\n",
        "section_key_word['policy - current news'] = 'polityka/aktualni-novyny'\n",
        "section_key_word['ukraine - on the frontline'] = 'ukrayina/na-liniyi-frontu'\n",
        "\n",
        "sample_link = sample_link_frontline_news[-1]+section_key_word['ukraine - on the frontline']\n",
        "sample_link = 'https://web.archive.org/web/20230131051233/http://vechirka.pl.ua/ukrayina/na-liniyi-frontu'\n",
        "print(sample_link)"
      ],
      "metadata": {
        "id": "QEMqqgjCHEM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bb29790-c4d3-49f0-ee25-7c07433b1a24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://web.archive.org/web/20230131051233/http://vechirka.pl.ua/ukrayina/na-liniyi-frontu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vechirka_links(base_url):\n",
        "    \"\"\"\n",
        "    Scrape all relevant links from the archived vechirka.pl.ua website.\n",
        "    Returns a list of dictionaries with link text and URL.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        #time.sleep(1)  # Be polite to the server\n",
        "        print('Getting the request.')\n",
        "        response = requests.get(base_url, headers=headers)\n",
        "        print('Got the request.')\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        print('Get soup.')\n",
        "\n",
        "        bookmark_links = []\n",
        "        for link in soup.find_all('a'):\n",
        "            print('a')\n",
        "            href = link.get('href', '')\n",
        "            link_text = link.text.strip()\n",
        "\n",
        "            # Check if the href matches the pattern we're looking for (ends with a digit and contains \"ukrayina\")\n",
        "            if \"ukrayina\" in href and len(link_text) > 0 and href[-1] in \"0123456789\":\n",
        "                # Extract archive timestamp from base_url\n",
        "                archive_timestamp = re.search(r'web/(\\d+)/', base_url).group(1) if re.search(r'web/(\\d+)/', base_url) else \"20220203004906\"\n",
        "\n",
        "                # Create proper archive URL\n",
        "                full_url = f\"https://web.archive.org/web/{archive_timestamp}/http://vechirka.pl.ua{href}\"\n",
        "                article = (link_text, href)\n",
        "                bookmark_links.append(article)\n",
        "\n",
        "        return bookmark_links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting vechirka links: {str(e)}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "T3kIqFS7b9vG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import time\n",
        "from typing import List, Tuple\n",
        "\n",
        "# Selenium Approach\n",
        "def get_vechirka_links_selenium(base_url: str) -> List[Tuple[str, str]]:\n",
        "    from selenium import webdriver\n",
        "    from selenium.webdriver.chrome.service import Service\n",
        "    from selenium.webdriver.chrome.options import Options\n",
        "    from selenium.webdriver.common.by import By\n",
        "    from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "    try:\n",
        "        # Configure Chrome options\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument(\"--headless\")  # Run in background\n",
        "        chrome_options.add_argument(\"--no-sandbox\")\n",
        "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "        # Setup WebDriver\n",
        "        service = Service(ChromeDriverManager().install())\n",
        "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "        # Navigate to page\n",
        "        driver.get(base_url)\n",
        "        time.sleep(2)  # Wait for page to load\n",
        "\n",
        "        # Find links\n",
        "        bookmark_links = []\n",
        "        links = driver.find_elements(By.TAG_NAME, 'a')\n",
        "\n",
        "        for link in links:\n",
        "            href = link.get_attribute('href') or ''\n",
        "            link_text = link.text.strip()\n",
        "\n",
        "            # Check if the href matches the pattern we're looking for\n",
        "            if \"ukrayina\" in href and len(link_text) > 0 and href[-1] in \"0123456789\":\n",
        "                # Extract archive timestamp from base_url\n",
        "                archive_timestamp = re.search(r'web/(\\d+)/', base_url).group(1) if re.search(r'web/(\\d+)/', base_url) else \"20220203004906\"\n",
        "\n",
        "                # Create proper archive URL\n",
        "                full_url = f\"https://web.archive.org/web/{archive_timestamp}/http://vechirka.pl.ua{href}\"\n",
        "                article = (link_text, href)\n",
        "                bookmark_links.append(article)\n",
        "\n",
        "        driver.quit()\n",
        "        return bookmark_links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Selenium Scraping Error: {str(e)}\")\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "buPrlJ5cXei8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_article_content(article_url):\n",
        "    \"\"\"\n",
        "    Visit an article page and extract the content.\n",
        "    For vechirka.pl.ua, we'll look for the main article content.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        time.sleep(2)  # Longer delay for article pages to be extra polite\n",
        "        response = requests.get(article_url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # First try to find content in a div with class field-item even (like in Chayka)\n",
        "        article_div = soup.find_all('div', class_='field-item even')\n",
        "\n",
        "        paragraphs = []\n",
        "        # for x in article_div:\n",
        "        #     print(x)\n",
        "        for p in article_div:\n",
        "        # for p in article_div.find_all('p'):\n",
        "            paragraphs.append(p.get_text(strip=False))\n",
        "            print('p')\n",
        "\n",
        "\n",
        "        # Join paragraphs with double newlines to preserve structure\n",
        "        content = \"\\n\\n\".join(paragraphs)\n",
        "\n",
        "        # If no paragraphs were found, get all text from the div\n",
        "        if not content:\n",
        "            content = article_div.get_text(strip=True)\n",
        "        return content\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting article: {str(e)}\"\n",
        "\n",
        "def save_to_file(title, content):\n",
        "    \"\"\"Save the scraped content to a TXT file\"\"\"\n",
        "    filename = title + '_English' + '.txt'\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write('Title: ' + title + '\\n')\n",
        "        # If 'content' is a list of strings, write each string on a new line\n",
        "        if isinstance(content, list):\n",
        "            for line in content:\n",
        "                f.write(f\"{line}\\n\")\n",
        "        # If 'content' is a single string, write it directly\n",
        "        elif isinstance(content, str):\n",
        "            f.write(content)\n",
        "        else:\n",
        "            raise ValueError(\"Content must be a string or a list of strings\")\n",
        "    print(f\"Content saved to {filename}\")"
      ],
      "metadata": {
        "id": "6_3u-hsRusSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import httpx\n",
        "import re\n",
        "from selectolax.parser import HTMLParser\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def get_vechirka_links_selectolax(base_url):\n",
        "    \"\"\"\n",
        "    Scrape all relevant links from the archived vechirka.pl.ua website.\n",
        "    Returns a list of dictionaries with link text and URL.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        print('Fetching the page...')\n",
        "        response = httpx.get(base_url, headers=headers, timeout=60)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        tree = HTMLParser(response.text)\n",
        "        print('Parsing the page...')\n",
        "\n",
        "        # Extract archive timestamp from base_url\n",
        "        match = re.search(r'web/(\\d+)/', base_url)\n",
        "        archive_timestamp = match.group(1) if match else \"20220203004906\"\n",
        "\n",
        "        bookmark_links = []\n",
        "        for node in tree.css('a'):\n",
        "            href = node.attributes.get('href', '')\n",
        "            link_text = node.text(strip=True)\n",
        "\n",
        "            # Check if URL contains \"ukrayina\" and ends with a digit\n",
        "            if \"ukrayina\" in href and href[-1].isdigit() and link_text:\n",
        "                full_url = urljoin(f\"https://web.archive.org/web/{archive_timestamp}/http://vechirka.pl.ua\", href)\n",
        "                bookmark_links.append((link_text, full_url))\n",
        "\n",
        "        print(f'Found {len(bookmark_links)} links.')\n",
        "        return bookmark_links\n",
        "\n",
        "    except httpx.HTTPError as e:\n",
        "        print(f\"Error fetching vechirka links: {str(e)}\")\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "GiJDJnT1YvSO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "2aefdf7b-89ee-4eb9-e157-9d43f59f6383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'selectolax'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-0a0d812223d1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mselectolax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTMLParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murljoin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'selectolax'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vechirka_links_helium(base_url: str) -> List[Tuple[str, str]]:\n",
        "    from helium import start_chrome, find_all, kill_browser\n",
        "\n",
        "    try:\n",
        "        # Start browser\n",
        "        driver = start_chrome(base_url)\n",
        "\n",
        "        # Find links\n",
        "        bookmark_links = []\n",
        "        links = find_all('a')\n",
        "\n",
        "        for link in links:\n",
        "            href = link.get_attribute('href') or ''\n",
        "            link_text = link.text.strip()\n",
        "\n",
        "            # Check if the href matches the pattern we're looking for\n",
        "            if \"ukrayina\" in href and len(link_text) > 0 and href[-1] in \"0123456789\":\n",
        "                # Extract archive timestamp from base_url\n",
        "                archive_timestamp = re.search(r'web/(\\d+)/', base_url).group(1) if re.search(r'web/(\\d+)/', base_url) else \"20220203004906\"\n",
        "\n",
        "                # Create proper archive URL\n",
        "                full_url = f\"https://web.archive.org/web/{archive_timestamp}/http://vechirka.pl.ua{href}\"\n",
        "                article = (link_text, href)\n",
        "                bookmark_links.append(article)\n",
        "\n",
        "        kill_browser()\n",
        "        return bookmark_links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Helium Scraping Error: {str(e)}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "jGYoT_V-aJca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install helium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRmXOkWQaLah",
        "outputId": "b8e86b11-de5e-464a-b6da-cd80119ccd2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting helium\n",
            "  Downloading helium-5.1.1.tar.gz (40 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: selenium>=4.16.0 in /usr/local/lib/python3.11/dist-packages (from helium) (4.30.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium>=4.16.0->helium) (2.3.0)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.11/dist-packages (from selenium>=4.16.0->helium) (0.29.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.11/dist-packages (from selenium>=4.16.0->helium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium>=4.16.0->helium) (2025.1.31)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium>=4.16.0->helium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium>=4.16.0->helium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium>=4.16.0->helium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium>=4.16.0->helium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium>=4.16.0->helium) (2.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium>=4.16.0->helium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium>=4.16.0->helium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.9->selenium>=4.16.0->helium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium>=4.16.0->helium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.16.0->helium) (0.9.0)\n",
            "Building wheels for collected packages: helium\n",
            "  Building wheel for helium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for helium: filename=helium-5.1.1-py3-none-any.whl size=41058 sha256=3520aeafa474cb9936df40dcc65cf59124d10917a70da28c5deda9d8f0597304\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/4d/b0/698033cd12ab8041849e98bab1c4db30879ea8250a8e8d75c4\n",
            "Successfully built helium\n",
            "Installing collected packages: helium\n",
            "Successfully installed helium-5.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = {}\n",
        "#bookmark_links = get_vechirka_links(sample_link)\n",
        "#bookmark_links = get_vechirka_links_selenium(sample_link)\n",
        "print(sample_link)\n",
        "#bookmark_links = get_vechirka_links_selectolax(sample_link)\n",
        "bookmark_links = get_vechirka_links_helium(sample_link)\n",
        "print(bookmark_links)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-trOg7PA2aq",
        "outputId": "832f85c2-326e-485d-d6f5-a299d1be3088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://web.archive.org/web/20230131051233/http://vechirka.pl.ua/ukrayina/na-liniyi-frontu\n",
            "Helium Scraping Error: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir\n",
            "Stacktrace:\n",
            "#0 0x5bfc606f9ffa <unknown>\n",
            "#1 0x5bfc601b8970 <unknown>\n",
            "#2 0x5bfc601f242a <unknown>\n",
            "#3 0x5bfc601ee18f <unknown>\n",
            "#4 0x5bfc6023ebd9 <unknown>\n",
            "#5 0x5bfc6023e106 <unknown>\n",
            "#6 0x5bfc60230063 <unknown>\n",
            "#7 0x5bfc601fc328 <unknown>\n",
            "#8 0x5bfc601fd491 <unknown>\n",
            "#9 0x5bfc606c142b <unknown>\n",
            "#10 0x5bfc606c52ec <unknown>\n",
            "#11 0x5bfc606a8a22 <unknown>\n",
            "#12 0x5bfc606c5e64 <unknown>\n",
            "#13 0x5bfc6068cbef <unknown>\n",
            "#14 0x5bfc606e8558 <unknown>\n",
            "#15 0x5bfc606e8736 <unknown>\n",
            "#16 0x5bfc606f8e76 <unknown>\n",
            "#17 0x7a47c54e1ac3 <unknown>\n",
            "\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in bookmark_links:\n",
        "    if x[0] != None:\n",
        "      title = translate_to_english(x[0])\n",
        "      if title == \"Read the article\" or title in '0123456789' or 'Next ›' in title or 'Last ”' in title: continue\n",
        "      url = '/'.join(x[1].split(\"/\")[3:])\n",
        "      d[title] = url\n",
        "for key in d:\n",
        "  print(f'Title: {key}, link: {d[key]}')"
      ],
      "metadata": {
        "id": "ZbXIflzvC7el"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_to_english(ukrainian_text):\n",
        "    translator = Translator()\n",
        "    english_translation = translator.translate(ukrainian_text, src='uk', dest='en').text\n",
        "    #english_translation = translator.translate(ukrainian_text, dest='en').text\n",
        "    return english_translation\n",
        "\n",
        "translate_to_english('\\nОлександр Корнієнко більше не буде головою партії «Слуга Народу». Це може стати початком періоду хитавиці в партії і напруження серед основних її членів, але багато чого тут залежатиме від того, хто тепер очолить партію. 37-річний Олександр Корнієнко став номінальним головним «слугою» (ніхто навіть не сумнівається, що реальним партійним лідером був і залишиться Президент Володимир Зеленський) у листопаді 2019 року. Тепер, рівно за два роки, він полишає важливий пост в ієрархії')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "il65W7fNGbhA",
        "outputId": "cbb5e620-986a-4fe1-dbce-3dac42c9e3ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Oleksandr Kornienko will no longer be the chairman of the party \"Servant of the People\".This can be the beginning of the rowing period in the party and the tension among its main members, but much will depend on the one who will now head the party.37-year-old Alexander Kornienko became a nominal chief \"servant\" (no one even doubts that President Volodymyr Zelenskyy was and will remain the real party leader in November 2019.Now, exactly two years, he leaves an important post in the hierarchy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_to_file(d, max_num_article):\n",
        "  i = 0\n",
        "  for title, url in d.items():\n",
        "    i += 1\n",
        "    if i > max_num_article: break\n",
        "    content = extract_article_content(url)\n",
        "    content = ''.join(content.split( )[:])\n",
        "    content = translate_to_english(content)\n",
        "    save_to_file(title, content)\n",
        "\n",
        "extract_to_file(d, 5)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "Z5C6chEvu2Os",
        "outputId": "c3891960-7a1f-479b-e27e-4c6a033a64ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Олександр\n",
            "Content saved to Chemist, producer and party boss: I ... [scrapped].txt\n",
            "2 Міністерство\n",
            "Content saved to The Ministry of Health has expanded the list of professions, both ... [scrapped].txt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-babf1afab3f1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msave_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mextract_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-babf1afab3f1>\u001b[0m in \u001b[0;36mextract_to_file\u001b[0;34m(d, max_num_article)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_num_article\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_article_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-ac149e0807cf>\u001b[0m in \u001b[0;36mextract_article_content\u001b[0;34m(article_url)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Longer delay for article pages to be extra polite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}