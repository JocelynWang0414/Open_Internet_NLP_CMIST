{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5110dde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bertopic in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (0.17.0)\n",
      "Requirement already satisfied: sentence-transformers in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (4.1.0)\n",
      "Requirement already satisfied: umap-learn in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (0.5.7)\n",
      "Requirement already satisfied: hdbscan in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (0.8.40)\n",
      "Requirement already satisfied: plotly in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (6.2.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from bertopic) (2.2.1)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from bertopic) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from bertopic) (1.7.0)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from bertopic) (4.67.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from sentence-transformers) (4.46.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: scipy in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from sentence-transformers) (0.33.1)\n",
      "Requirement already satisfied: Pillow in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from sentence-transformers) (4.14.0)\n",
      "Requirement already satisfied: numba>=0.51.2 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from umap-learn) (0.61.2)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from umap-learn) (0.5.13)\n",
      "Requirement already satisfied: joblib>=1.0 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from hdbscan) (1.5.1)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from plotly) (1.44.0)\n",
      "Requirement already satisfied: packaging in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from plotly) (25.0)\n",
      "Requirement already satisfied: filelock in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from numba>=0.51.2->umap-learn) (0.44.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from pandas>=1.1.5->bertopic) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from pandas>=1.1.5->bertopic) (2024.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from scikit-learn>=1.0->bertopic) (3.6.0)\n",
      "Requirement already satisfied: setuptools in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install bertopic sentence-transformers umap-learn hdbscan plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfed8ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbformat in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (5.10.4)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from nbformat) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from nbformat) (4.24.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from nbformat) (5.8.1)\n",
      "Requirement already satisfied: traitlets>=5.1 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from nbformat) (5.14.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (0.25.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.3.8)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /Users/wangcancan/miniconda3/envs/py312/lib/python3.12/site-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat) (4.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3394e307",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\npartially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/utils/import_utils.py:1778\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1779\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:999\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/modeling_utils.py:48\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloss\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloss_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LOSS_MAPPING\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpytorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     50\u001b[39m     Conv1D,\n\u001b[32m     51\u001b[39m     apply_chunking_to_forward,\n\u001b[32m   (...)\u001b[39m\u001b[32m     57\u001b[39m     prune_linear_layer,\n\u001b[32m     58\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/loss/loss_utils.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BCEWithLogitsLoss, MSELoss\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloss_deformable_detr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeformableDetrForObjectDetectionLoss, DeformableDetrForSegmentationLoss\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloss_for_object_detection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ForObjectDetectionLoss, ForSegmentationLoss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/loss/loss_deformable_detr.py:4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage_transforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m center_to_corners_format\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_scipy_available\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/image_transforms.py:22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     23\u001b[39m     ChannelDimension,\n\u001b[32m     24\u001b[39m     ImageInput,\n\u001b[32m     25\u001b[39m     get_channel_dimension_axis,\n\u001b[32m     26\u001b[39m     get_image_size,\n\u001b[32m     27\u001b[39m     infer_channel_dimension_format,\n\u001b[32m     28\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExplicitEnum, TensorType, is_jax_tensor, is_tf_tensor, is_torch_tensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/image_utils.py:58\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torchvision_available():\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InterpolationMode\n\u001b[32m     60\u001b[39m     pil_torch_interpolation_mapping = {\n\u001b[32m     61\u001b[39m         PILImageResampling.NEAREST: InterpolationMode.NEAREST,\n\u001b[32m     62\u001b[39m         PILImageResampling.BOX: InterpolationMode.BOX,\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m         PILImageResampling.LANCZOS: InterpolationMode.LANCZOS,\n\u001b[32m     67\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torchvision/__init__.py:10\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torchvision/_meta_registrations.py:25\u001b[39m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;129;43m@register_meta\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mroi_align\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43mmeta_roi_align\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maligned\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrois\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrois must have shape as Tensor[K, 5]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torchvision/_meta_registrations.py:18\u001b[39m, in \u001b[36mregister_meta.<locals>.wrapper\u001b[39m\u001b[34m(fn)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(fn):\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorchvision\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextension\u001b[49m._has_ops():\n\u001b[32m     19\u001b[39m         get_meta_lib().impl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch.ops.torchvision, op_name), overload_name), fn)\n",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# BERTopic and related imports\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbertopic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BERTopic\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mumap\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UMAP\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/bertopic/__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbertopic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_bertopic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BERTopic\n\u001b[32m      5\u001b[39m __version__ = version(\u001b[33m\"\u001b[39m\u001b[33mbertopic\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m __all__ = [\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mBERTopic\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/bertopic/_bertopic.py:58\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbertopic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m plotting\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbertopic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseCluster\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbertopic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseEmbedder\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbertopic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrepresentation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_mmr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mmr\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbertopic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m select_backend\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/bertopic/backend/__init__.py:22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Multimodal Embeddings\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbertopic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_multimodal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiModalBackend\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[32m     24\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33m`pip install bertopic[vision]` \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/bertopic/backend/_multimodal.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Union\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbertopic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseEmbedder\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMultiModalBackend\u001b[39;00m(BaseEmbedder):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/sentence_transformers/__init__.py:14\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     10\u001b[39m     export_dynamic_quantized_onnx_model,\n\u001b[32m     11\u001b[39m     export_optimized_onnx_model,\n\u001b[32m     12\u001b[39m     export_static_quantized_openvino_model,\n\u001b[32m     13\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcross_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     15\u001b[39m     CrossEncoder,\n\u001b[32m     16\u001b[39m     CrossEncoderModelCardData,\n\u001b[32m     17\u001b[39m     CrossEncoderTrainer,\n\u001b[32m     18\u001b[39m     CrossEncoderTrainingArguments,\n\u001b[32m     19\u001b[39m )\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mLoggingHandler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/sentence_transformers/cross_encoder/__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mCrossEncoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_card\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderModelCardData\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautonotebook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m trange\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     AutoConfig,\n\u001b[32m     21\u001b[39m     AutoModelForSequenceClassification,\n\u001b[32m     22\u001b[39m     AutoTokenizer,\n\u001b[32m     23\u001b[39m     PretrainedConfig,\n\u001b[32m     24\u001b[39m     PreTrainedModel,\n\u001b[32m     25\u001b[39m     PreTrainedTokenizer,\n\u001b[32m     26\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PushToHubMixin\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deprecated\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/utils/import_utils.py:1766\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1764\u001b[39m     value = Placeholder\n\u001b[32m   1765\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m-> \u001b[39m\u001b[32m1766\u001b[39m     module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1767\u001b[39m     value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   1768\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/utils/import_utils.py:1780\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   1779\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1780\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1781\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m because of the following error (look up to see its\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1782\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1783\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\npartially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# BERTopic and related imports\n",
    "try:\n",
    "    from bertopic import BERTopic\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from umap import UMAP\n",
    "    from hdbscan import HDBSCAN\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    BERTOPIC_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"BERTopic dependencies not found. Please install with:\")\n",
    "    print(\"pip install bertopic sentence-transformers umap-learn hdbscan\")\n",
    "    BERTOPIC_AVAILABLE = False\n",
    "\n",
    "# Alternative lightweight implementation for environments without BERTopic\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bfa5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTopicPipeline:\n",
    "    \"\"\"\n",
    "    Comprehensive BERTopic modeling pipeline for analyzing topics in text collections.\n",
    "    Handles short to medium texts (80-400 characters) effectively.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model='all-MiniLM-L6-v2', language='english', random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize BERTopic pipeline.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: Sentence transformer model name\n",
    "            language: Language for preprocessing\n",
    "            random_state: Random state for reproducibility\n",
    "        \"\"\"\n",
    "        self.embedding_model_name = embedding_model\n",
    "        self.language = language\n",
    "        self.random_state = random_state\n",
    "        self.topic_model = None\n",
    "        self.embeddings = None\n",
    "        self.texts = None\n",
    "        self.topics = None\n",
    "        self.topic_info = None\n",
    "        \n",
    "        if BERTOPIC_AVAILABLE:\n",
    "            # Initialize embedding model\n",
    "            self.embedding_model = SentenceTransformer(embedding_model)\n",
    "            \n",
    "            # Initialize UMAP for dimensionality reduction\n",
    "            self.umap_model = UMAP(\n",
    "                n_neighbors=5,          # Increase neighbors\n",
    "                n_components=5,          # Use more components initially\n",
    "                min_dist=0.0,\n",
    "                metric='cosine',\n",
    "                random_state=42,\n",
    "                init='random'            # Use random initialization instead of spectral\n",
    "            )\n",
    "            \n",
    "            # Initialize HDBSCAN for clustering\n",
    "            self.hdbscan_model = HDBSCAN(\n",
    "                min_cluster_size=5,\n",
    "                metric='euclidean',\n",
    "                cluster_selection_method='eom',\n",
    "                prediction_data=True\n",
    "            )\n",
    "        else:\n",
    "            print(\"Using lightweight fallback implementation\")\n",
    "    \n",
    "    def preprocess_texts(self, texts):\n",
    "        \"\"\"\n",
    "        Basic preprocessing for texts.\n",
    "        \"\"\"\n",
    "        processed_texts = []\n",
    "        for text in texts:\n",
    "            # Basic cleaning\n",
    "            text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "            text = re.sub(r'\\S+@\\S+', '', text)\n",
    "            text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            \n",
    "            if len(text) > 10:  # Keep texts with reasonable length\n",
    "                processed_texts.append(text)\n",
    "        \n",
    "        return processed_texts\n",
    "    \n",
    "    def fit_bertopic(self, texts, nr_topics='auto', min_topic_size=3):\n",
    "        \"\"\"\n",
    "        Fit BERTopic model on the provided texts.\n",
    "        \"\"\"\n",
    "        if not BERTOPIC_AVAILABLE:\n",
    "            return self._fit_fallback(texts, nr_topics)\n",
    "        \n",
    "        print(\"Preprocessing texts...\")\n",
    "        self.texts = self.preprocess_texts(texts)\n",
    "        print(f\"Processing {len(self.texts)} texts...\")\n",
    "        \n",
    "        # Create embeddings\n",
    "        print(\"Creating embeddings...\")\n",
    "        self.embeddings = self.embedding_model.encode(self.texts, show_progress_bar=True)\n",
    "        \n",
    "        # Adjust clustering parameters based on text count\n",
    "        if len(self.texts) < 50:\n",
    "            self.hdbscan_model.min_cluster_size = max(2, len(self.texts) // 20)\n",
    "            self.umap_model.n_neighbors = min(10, len(self.texts) // 3)\n",
    "        \n",
    "        # Create custom vectorizer for better topic representation\n",
    "        vectorizer_model = CountVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words='english',\n",
    "            min_df=1,\n",
    "            max_df=0.9\n",
    "        )\n",
    "        \n",
    "        # Initialize BERTopic with custom components\n",
    "        self.topic_model = BERTopic(\n",
    "            embedding_model=self.embedding_model,\n",
    "            umap_model=self.umap_model,\n",
    "            hdbscan_model=self.hdbscan_model,\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            nr_topics=nr_topics,\n",
    "            calculate_probabilities=True,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Fit the model\n",
    "        print(\"Fitting BERTopic model...\")\n",
    "        self.topics, self.probabilities = self.topic_model.fit_transform(self.texts, self.embeddings)\n",
    "        \n",
    "        # Get topic information\n",
    "        self.topic_info = self.topic_model.get_topic_info()\n",
    "        \n",
    "        print(f\"Model fitted successfully!\")\n",
    "        print(f\"Number of topics found: {len(self.topic_info) - 1}\")  # -1 for outlier topic\n",
    "        print(f\"Number of outliers: {sum(1 for t in self.topics if t == -1)}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _fit_fallback(self, texts, nr_topics):\n",
    "        \"\"\"\n",
    "        Fallback implementation when BERTopic is not available.\n",
    "        \"\"\"\n",
    "        print(\"Using fallback topic modeling (TF-IDF + K-Means)...\")\n",
    "        \n",
    "        self.texts = self.preprocess_texts(texts)\n",
    "        \n",
    "        # Use TF-IDF for embeddings\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=1000,\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words='english',\n",
    "            min_df=1,\n",
    "            max_df=0.9\n",
    "        )\n",
    "        \n",
    "        tfidf_matrix = vectorizer.fit_transform(self.texts)\n",
    "        \n",
    "        # Reduce dimensionality\n",
    "        svd = TruncatedSVD(n_components=min(50, len(self.texts)-1), random_state=self.random_state)\n",
    "        self.embeddings = svd.fit_transform(tfidf_matrix)\n",
    "        \n",
    "        # Determine number of topics\n",
    "        if nr_topics == 'auto':\n",
    "            nr_topics = min(10, max(2, len(self.texts) // 10))\n",
    "        elif isinstance(nr_topics, str):\n",
    "            nr_topics = 5\n",
    "        \n",
    "        # Cluster documents\n",
    "        kmeans = KMeans(n_clusters=nr_topics, random_state=self.random_state)\n",
    "        self.topics = kmeans.fit_predict(self.embeddings)\n",
    "        \n",
    "        # Create topic info\n",
    "        self.topic_info = self._create_fallback_topic_info(vectorizer, tfidf_matrix)\n",
    "        \n",
    "        print(f\"Fallback model fitted with {nr_topics} topics\")\n",
    "        return self\n",
    "    \n",
    "    def _create_fallback_topic_info(self, vectorizer, tfidf_matrix):\n",
    "        \"\"\"\n",
    "        Create topic information for fallback implementation.\n",
    "        \"\"\"\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        topic_info = []\n",
    "        \n",
    "        for topic_id in range(max(self.topics) + 1):\n",
    "            # Get documents in this topic\n",
    "            topic_docs = [i for i, t in enumerate(self.topics) if t == topic_id]\n",
    "            \n",
    "            if topic_docs:\n",
    "                # Calculate average TF-IDF for this topic\n",
    "                topic_tfidf = tfidf_matrix[topic_docs].mean(axis=0).A1\n",
    "                \n",
    "                # Get top words\n",
    "                top_indices = topic_tfidf.argsort()[-10:][::-1]\n",
    "                top_words = [feature_names[i] for i in top_indices]\n",
    "                \n",
    "                topic_info.append({\n",
    "                    'Topic': topic_id,\n",
    "                    'Count': len(topic_docs),\n",
    "                    'Name': '_'.join(top_words[:3]),\n",
    "                    'Representation': top_words\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(topic_info)\n",
    "    \n",
    "    def get_topic_info(self):\n",
    "        \"\"\"\n",
    "        Get information about discovered topics.\n",
    "        \"\"\"\n",
    "        if self.topic_info is None:\n",
    "            raise ValueError(\"Model not fitted. Please fit the model first.\")\n",
    "        \n",
    "        return self.topic_info\n",
    "    \n",
    "    def get_topic_words(self, topic_id, n_words=10):\n",
    "        \"\"\"\n",
    "        Get top words for a specific topic.\n",
    "        \"\"\"\n",
    "        if not BERTOPIC_AVAILABLE:\n",
    "            if topic_id < len(self.topic_info):\n",
    "                return self.topic_info.iloc[topic_id]['Representation'][:n_words]\n",
    "            return []\n",
    "        \n",
    "        if self.topic_model is None:\n",
    "            raise ValueError(\"Model not fitted. Please fit the model first.\")\n",
    "        \n",
    "        return [word for word, _ in self.topic_model.get_topic(topic_id)[:n_words]]\n",
    "    \n",
    "    def get_document_topics(self, texts=None, threshold=0.1):\n",
    "        \"\"\"\n",
    "        Get topic assignments for documents.\n",
    "        \"\"\"\n",
    "        if self.topics is None:\n",
    "            raise ValueError(\"Model not fitted. Please fit the model first.\")\n",
    "        \n",
    "        if texts is None:\n",
    "            texts = self.texts\n",
    "        \n",
    "        doc_topics = []\n",
    "        for i, (text, topic) in enumerate(zip(texts, self.topics)):\n",
    "            topic_prob = 1.0  # Default probability\n",
    "            \n",
    "            if BERTOPIC_AVAILABLE and hasattr(self, 'probabilities') and self.probabilities is not None:\n",
    "                if i < len(self.probabilities):\n",
    "                    topic_probs = self.probabilities[i]\n",
    "                    if topic >= 0 and topic < len(topic_probs):\n",
    "                        topic_prob = topic_probs[topic]\n",
    "            \n",
    "            doc_topics.append({\n",
    "                'text': text,\n",
    "                'topic': topic,\n",
    "                'probability': topic_prob,\n",
    "                'is_outlier': topic == -1\n",
    "            })\n",
    "        \n",
    "        return doc_topics\n",
    "    \n",
    "    def visualize_topics(self, width=800, height=600):\n",
    "        \"\"\"\n",
    "        Create interactive topic visualization.\n",
    "        \"\"\"\n",
    "        if BERTOPIC_AVAILABLE and self.topic_model is not None:\n",
    "            # Use BERTopic's built-in visualization\n",
    "            fig = self.topic_model.visualize_topics(width=width, height=height)\n",
    "            fig.show()\n",
    "        else:\n",
    "            # Fallback visualization\n",
    "            self._visualize_topics_fallback()\n",
    "    \n",
    "    def _visualize_topics_fallback(self):\n",
    "        \"\"\"\n",
    "        Fallback visualization using matplotlib.\n",
    "        \"\"\"\n",
    "        if self.embeddings is None:\n",
    "            print(\"No embeddings available for visualization\")\n",
    "            return\n",
    "        \n",
    "        # Reduce to 2D for visualization\n",
    "        tsne = TSNE(n_components=2, random_state=self.random_state, perplexity=min(30, len(self.texts)//4))\n",
    "        embeddings_2d = tsne.fit_transform(self.embeddings)\n",
    "        \n",
    "        # Create scatter plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                            c=self.topics, cmap='tab10', alpha=0.7)\n",
    "        plt.colorbar(scatter, label='Topic')\n",
    "        plt.title('Topic Visualization (t-SNE)')\n",
    "        plt.xlabel('Dimension 1')\n",
    "        plt.ylabel('Dimension 2')\n",
    "        \n",
    "        # Add topic labels\n",
    "        for topic_id in range(max(self.topics) + 1):\n",
    "            topic_points = embeddings_2d[np.array(self.topics) == topic_id]\n",
    "            if len(topic_points) > 0:\n",
    "                centroid = np.mean(topic_points, axis=0)\n",
    "                plt.annotate(f'Topic {topic_id}', centroid,\n",
    "                           xytext=(5, 5), textcoords='offset points',\n",
    "                           bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def visualize_documents(self, custom_labels=None):\n",
    "        \"\"\"\n",
    "        Visualize documents in topic space.\n",
    "        \"\"\"\n",
    "        if BERTOPIC_AVAILABLE and self.topic_model is not None:\n",
    "            fig = self.topic_model.visualize_documents(\n",
    "                self.texts,\n",
    "                custom_labels=custom_labels,\n",
    "                width=1000,\n",
    "                height=700\n",
    "            )\n",
    "            fig.show()\n",
    "        else:\n",
    "            print(\"Document visualization requires full BERTopic installation\")\n",
    "    \n",
    "    def visualize_hierarchy(self):\n",
    "        \"\"\"\n",
    "        Visualize topic hierarchy.\n",
    "        \"\"\"\n",
    "        if BERTOPIC_AVAILABLE and self.topic_model is not None:\n",
    "            fig = self.topic_model.visualize_hierarchy()\n",
    "            fig.show()\n",
    "        else:\n",
    "            print(\"Hierarchy visualization requires full BERTopic installation\")\n",
    "    \n",
    "    def visualize_barchart(self, topics=None, n_words=8):\n",
    "        \"\"\"\n",
    "        Create bar chart of top words per topic.\n",
    "        \"\"\"\n",
    "        if BERTOPIC_AVAILABLE and self.topic_model is not None:\n",
    "            fig = self.topic_model.visualize_barchart(topics=topics, n_words=n_words)\n",
    "            fig.show()\n",
    "        else:\n",
    "            self._visualize_barchart_fallback(n_words)\n",
    "    \n",
    "    def _visualize_barchart_fallback(self, n_words=8):\n",
    "        \"\"\"\n",
    "        Fallback bar chart visualization.\n",
    "        \"\"\"\n",
    "        if self.topic_info is None:\n",
    "            return\n",
    "        \n",
    "        n_topics = min(6, len(self.topic_info))\n",
    "        fig, axes = plt.subplots((n_topics + 1) // 2, 2, figsize=(15, 4 * ((n_topics + 1) // 2)))\n",
    "        if n_topics == 1:\n",
    "            axes = [axes]\n",
    "        elif (n_topics + 1) // 2 == 1:\n",
    "            axes = [axes]\n",
    "        else:\n",
    "            axes = axes.flatten()\n",
    "        \n",
    "        for i in range(n_topics):\n",
    "            if i < len(self.topic_info):\n",
    "                words = self.topic_info.iloc[i]['Representation'][:n_words]\n",
    "                values = [1.0 - j*0.1 for j in range(len(words))]  # Dummy values\n",
    "                \n",
    "                axes[i].barh(range(len(words)), values)\n",
    "                axes[i].set_yticks(range(len(words)))\n",
    "                axes[i].set_yticklabels(words)\n",
    "                axes[i].set_title(f'Topic {i}')\n",
    "                axes[i].invert_yaxis()\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for i in range(n_topics, len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def search_topics(self, search_term, top_k=5):\n",
    "        \"\"\"\n",
    "        Search for topics related to a search term.\n",
    "        \"\"\"\n",
    "        if BERTOPIC_AVAILABLE and self.topic_model is not None:\n",
    "            similar_topics, similarity_scores = self.topic_model.find_topics(search_term, top_n=top_k)\n",
    "            return list(zip(similar_topics, similarity_scores))\n",
    "        else:\n",
    "            # Fallback search\n",
    "            search_results = []\n",
    "            for i, row in self.topic_info.iterrows():\n",
    "                topic_words = ' '.join(row['Representation'])\n",
    "                if search_term.lower() in topic_words.lower():\n",
    "                    search_results.append((row['Topic'], 1.0))  # Dummy similarity\n",
    "            return search_results[:top_k]\n",
    "    \n",
    "    def get_topic_evolution(self, timestamps=None, nr_bins=10):\n",
    "        \"\"\"\n",
    "        Analyze topic evolution over time (if timestamps provided).\n",
    "        \"\"\"\n",
    "        if timestamps is None or not BERTOPIC_AVAILABLE:\n",
    "            print(\"Topic evolution requires timestamps and full BERTopic installation\")\n",
    "            return None\n",
    "        \n",
    "        if self.topic_model is not None:\n",
    "            topics_over_time = self.topic_model.topics_over_time(\n",
    "                self.texts, timestamps, nr_bins=nr_bins\n",
    "            )\n",
    "            return topics_over_time\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"\n",
    "        Save the trained model.\n",
    "        \"\"\"\n",
    "        if BERTOPIC_AVAILABLE and self.topic_model is not None:\n",
    "            self.topic_model.save(filepath)\n",
    "            print(f\"Model saved to {filepath}\")\n",
    "        else:\n",
    "            print(\"Model saving requires full BERTopic installation\")\n",
    "    \n",
    "    def export_results(self, filename='bertopic_results.csv'):\n",
    "        \"\"\"\n",
    "        Export topic modeling results to CSV.\n",
    "        \"\"\"\n",
    "        doc_topics = self.get_document_topics()\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        results_df = pd.DataFrame([\n",
    "            {\n",
    "                'text': doc['text'],\n",
    "                'topic': doc['topic'],\n",
    "                'probability': doc['probability'],\n",
    "                'is_outlier': doc['is_outlier']\n",
    "            }\n",
    "            for doc in doc_topics\n",
    "        ])\n",
    "        \n",
    "        # Add topic information\n",
    "        topic_words = {}\n",
    "        for i in range(max(self.topics) + 1):\n",
    "            words = self.get_topic_words(i, n_words=5)\n",
    "            topic_words[i] = ', '.join(words) if words else ''\n",
    "        \n",
    "        results_df['topic_words'] = results_df['topic'].map(topic_words)\n",
    "        \n",
    "        # Save to CSV\n",
    "        results_df.to_csv(filename, index=False)\n",
    "        \n",
    "        # Save topic summary\n",
    "        topic_summary = self.get_topic_info()\n",
    "        topic_summary.to_csv(f'topic_summary_{filename}', index=False)\n",
    "        \n",
    "        print(f\"Results exported to {filename}\")\n",
    "        return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a06edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_bertopic():\n",
    "    \"\"\"\n",
    "    Demonstrate BERTopic pipeline with sample data.\n",
    "    \"\"\"\n",
    "    # Sample texts covering different topics\n",
    "    # sample_texts = [\n",
    "    #     \"Machine learning algorithms are revolutionizing healthcare diagnostics and treatment\",\n",
    "    #     \"Climate change is causing severe weather patterns and environmental damage\",\n",
    "    #     \"Stock markets are volatile due to economic uncertainty and inflation\",\n",
    "    #     \"Deep learning models like BERT are transforming natural language processing\",\n",
    "    #     \"Renewable energy sources are becoming more cost-effective than fossil fuels\",\n",
    "    #     \"Cryptocurrency adoption is growing despite regulatory challenges\",\n",
    "    #     \"Computer vision technology is improving autonomous vehicle safety\",\n",
    "    #     \"Global warming is melting polar ice caps and raising sea levels\",\n",
    "    #     \"Financial markets react strongly to central bank policy decisions\",\n",
    "    #     \"Neural networks are enabling breakthrough advances in AI research\",\n",
    "    #     \"Environmental conservation efforts focus on protecting endangered species\",\n",
    "    #     \"Investment portfolios need diversification to manage risk effectively\",\n",
    "    #     \"Natural language understanding enables better human-computer interaction\",\n",
    "    #     \"Climate policies aim to reduce carbon emissions and promote sustainability\",\n",
    "    #     \"Algorithmic trading is changing how financial markets operate\",\n",
    "    #     \"Artificial intelligence is transforming industries across the globe\",\n",
    "    #     \"Ocean temperatures are rising due to greenhouse gas emissions\",\n",
    "    #     \"Economic indicators suggest potential recession in major economies\",\n",
    "    #     \"Transformer models have revolutionized machine translation systems\",\n",
    "    #     \"Solar and wind energy are becoming dominant renewable technologies\",\n",
    "    #     \"Large language models are reshaping content creation and digital communication\",\n",
    "    #     \"Electric vehicles are driving innovation in battery technology and infrastructure\",\n",
    "    #     \"Facial recognition systems raise ethical concerns about privacy and surveillance\",\n",
    "    #     \"Quantum computing promises exponential speedup for complex problem solving\",\n",
    "    #     \"Global supply chain disruptions are affecting manufacturing and retail sectors\",\n",
    "    #     \"Gene editing technologies like CRISPR are transforming biotechnology research\",\n",
    "    #     \"Cybersecurity threats are increasing as critical infrastructure becomes digital\",\n",
    "    #     \"Data-driven agriculture improves crop yields and sustainability outcomes\",\n",
    "    #     \"Space exploration is accelerating through public-private sector collaboration\",\n",
    "    #     \"Biometric authentication is replacing traditional password-based security\",\n",
    "    #     \"AI ethics frameworks are emerging to address bias and accountability in systems\",\n",
    "    #     \"Telemedicine adoption is expanding healthcare access in rural communities\",\n",
    "    #     \"Decentralized finance platforms challenge traditional banking systems\",\n",
    "    #     \"Edge computing enables faster processing for IoT and real-time analytics\",\n",
    "    #     \"Satellite data is advancing climate monitoring and disaster prediction\",\n",
    "    #     \"Privacy-preserving machine learning protects sensitive user information\",\n",
    "    #     \"Robotic process automation streamlines repetitive enterprise workflows\",\n",
    "    #     \"Digital twins simulate real-world systems for better decision-making\",\n",
    "    #     \"5G networks facilitate high-speed connectivity for smart cities and devices\",\n",
    "    #     \"Augmented reality is transforming retail, education, and remote collaboration\",\n",
    "    #     \"Blockchain technology is enabling secure and transparent digital transactions\",\n",
    "    #     \"Artificial intelligence is optimizing logistics and supply chain operations\",\n",
    "    #     \"Microplastics pollution poses serious risks to marine ecosystems and human health\",\n",
    "    #     \"Virtual reality is enhancing simulation training in medicine and aviation\",\n",
    "    #     \"Green hydrogen is emerging as a key solution for clean energy storage\",\n",
    "    #     \"Machine learning accelerates drug discovery and personalized treatment plans\",\n",
    "    #     \"Social media algorithms influence public opinion and political discourse\",\n",
    "    #     \"Internet of Things devices are transforming home automation and smart living\",\n",
    "    #     \"Climate adaptation strategies are vital for coastal city resilience\",\n",
    "    #     \"Predictive analytics is improving maintenance in industrial manufacturing\",\n",
    "    #     \"Autonomous drones are revolutionizing delivery services and aerial monitoring\",\n",
    "    #     \"Remote work technologies are reshaping corporate culture and productivity\",\n",
    "    #     \"Carbon capture and storage could mitigate industrial greenhouse gas emissions\",\n",
    "    #     \"AI-generated content raises questions about authorship and misinformation\",\n",
    "    #     \"Smart grids help balance electricity demand and integrate renewable sources\",\n",
    "    #     \"Synthetic biology is engineering organisms for sustainable material production\",\n",
    "    #     \"Economic inequality is widening due to uneven access to digital resources\"\n",
    "    # ]\n",
    "    df = pd.read_csv('india2013.csv')\n",
    "    sample_texts = df['Paragraph'].tolist()\n",
    "\n",
    "    \n",
    "    print(\"BERTopic Pipeline Demo\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = BERTopicPipeline(embedding_model='all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Fit model\n",
    "    pipeline.fit_bertopic(sample_texts, nr_topics='auto')\n",
    "    \n",
    "    # Display topic information\n",
    "    print(\"\\nTopic Information:\")\n",
    "    print(\"=\" * 30)\n",
    "    topic_info = pipeline.get_topic_info()\n",
    "    print(topic_info)\n",
    "    \n",
    "    # Show top words for each topic\n",
    "    print(f\"\\nTop Words per Topic:\")\n",
    "    print(\"=\" * 30)\n",
    "    for topic_id in range(len(topic_info)):\n",
    "        if topic_id < max(pipeline.topics) + 1:\n",
    "            words = pipeline.get_topic_words(topic_id, n_words=6)\n",
    "            print(f\"Topic {topic_id}: {', '.join(words)}\")\n",
    "    \n",
    "    # Document topic assignments\n",
    "    doc_topics = pipeline.get_document_topics()\n",
    "    print(f\"\\nSample Document Assignments:\")\n",
    "    print(\"=\" * 35)\n",
    "    for i, doc in enumerate(doc_topics[:5]):\n",
    "        print(f\"Text: {doc['text'][:50]}...\")\n",
    "        print(f\"Topic: {doc['topic']} (prob: {doc['probability']:.3f})\")\n",
    "        print()\n",
    "    \n",
    "    # Visualizations\n",
    "    print(\"Creating visualizations...\")\n",
    "    # pipeline.visualize_topics()\n",
    "    pipeline.visualize_barchart()\n",
    "    \n",
    "    # Search functionality\n",
    "    print(f\"\\nTopic Search Results for 'machine learning':\")\n",
    "    search_results = pipeline.search_topics('machine learning', top_k=3)\n",
    "    for topic_id, score in search_results:\n",
    "        print(f\"Topic {topic_id}: {score:.3f}\")\n",
    "    \n",
    "    # Export results\n",
    "    results_df = pipeline.export_results('demo_bertopic_results.csv')\n",
    "    print(f\"\\nExported {len(results_df)} document results\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# Quick setup function for easy use\n",
    "def quick_bertopic_analysis(texts, nr_topics='auto', model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    Quick function to perform BERTopic analysis with minimal setup.\n",
    "    \"\"\"\n",
    "    pipeline = BERTopicPipeline(embedding_model=model_name)\n",
    "    pipeline.fit_bertopic(texts, nr_topics=nr_topics)\n",
    "    \n",
    "    # Print summary\n",
    "    topic_info = pipeline.get_topic_info()\n",
    "    print(f\"Found {len(topic_info)} topics in {len(texts)} documents\")\n",
    "    \n",
    "    for i, row in topic_info.iterrows():\n",
    "        if i < max(pipeline.topics) + 1:\n",
    "            words = pipeline.get_topic_words(i, n_words=5)\n",
    "            print(f\"Topic {i}: {', '.join(words)}\")\n",
    "    \n",
    "    pipeline.visualize_topics()\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f639106",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BERTOPIC_AVAILABLE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Check if BERTopic is available\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mBERTOPIC_AVAILABLE\u001b[49m:\n\u001b[32m      4\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFull BERTopic functionality available\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m         demo_pipeline = demo_bertopic()\n",
      "\u001b[31mNameError\u001b[39m: name 'BERTOPIC_AVAILABLE' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Check if BERTopic is available\n",
    "    if BERTOPIC_AVAILABLE:\n",
    "        print(\"Full BERTopic functionality available\")\n",
    "        demo_pipeline = demo_bertopic()\n",
    "    else:\n",
    "        print(\"Running with fallback implementation\")\n",
    "        print(\"To use full BERTopic features, install: pip install bertopic sentence-transformers umap-learn hdbscan\")\n",
    "        demo_pipeline = demo_bertopic()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
