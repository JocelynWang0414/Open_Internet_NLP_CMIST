{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38eaab51",
   "metadata": {},
   "source": [
    "# PDF/TXT_to_Sentence_Level_CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0241f80",
   "metadata": {},
   "source": [
    "## Marker AI PDF to Markdown File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e94b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.run([\"marker_single\", \"netherland2011.pdf\"], timeout=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc63892",
   "metadata": {},
   "source": [
    "## BeautifulSoup Markdown (HTML) to TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74b1d8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def markdown_to_text(md_path, txt_path):\n",
    "    # Read the markdown file\n",
    "    with open(md_path, 'r', encoding='utf-8') as md_file:\n",
    "        md_content = md_file.read()\n",
    "\n",
    "    # Convert markdown to HTML\n",
    "    html = markdown.markdown(md_content)\n",
    "\n",
    "    # Strip HTML tags to get plain text\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # Write the plain text to a .txt file\n",
    "    with open(txt_path, 'w', encoding='utf-8') as txt_file:\n",
    "        txt_file.write(text)\n",
    "\n",
    "# files = ['southafrica2015', 'china2016', 'india2013', 'netherland2011']\n",
    "# for filename in files:\n",
    "#     markdown_to_text(f\"{filename}/{filename}.md\", filename + \".txt\")\n",
    "\n",
    "filename = 'china2016'\n",
    "markdown_to_text(f\"{filename}/{filename}_clean.md\", filename + \"_cleaned.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83ba0c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.0-cp312-cp312-macosx_10_13_x86_64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.26.0 in /opt/miniconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.3.0-cp312-cp312-macosx_10_13_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.3.0 pytz-2025.2 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5fa991",
   "metadata": {},
   "source": [
    "## Text Cleaning Using Regex (symbols, numbers) + TXT to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f4659d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved as: china2016_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "files = ['india2013', 'china2016', 'southafrica2015', 'netherland2011']\n",
    "files = ['china2016_cleaned']\n",
    "for file in files:\n",
    "    # Input/output file paths\n",
    "    input_path = Path(f'./{file}.txt')\n",
    "    output_csv = Path(str(input_path)[:-4] + '.csv')\n",
    "\n",
    "    # Read and clean the raw text\n",
    "    text = input_path.read_text(encoding='utf-8')\n",
    "\n",
    "    # Remove bullet points, list markers, etc.\n",
    "    text = re.sub(r\"^\\s*[\\d\\.\\-\\–•]+\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove short lines (e.g., headings)\n",
    "    lines = [line.strip() for line in text.splitlines() if len(line.strip()) >= 5]\n",
    "\n",
    "    # Join lines into one string\n",
    "    text_combined = \" \".join(lines)\n",
    "    text_combined = re.sub(r'\\s+', ' ', text_combined)  # normalize whitespace\n",
    "\n",
    "    # Split into sentences\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text_combined)\n",
    "\n",
    "   # Compile patterns\n",
    "    allowed_pattern = re.compile(r\"[^A-Za-z\\s.,]\")  # only allow letters, whitespace, comma, and period\n",
    "    repeated_punct_pattern = re.compile(r\"[.,]{2,}\")  # remove repeated periods or commas\n",
    "    hyphen_linebreak_pattern = re.compile(r\"(\\w)-\\s+(\\w)\")  # join hyphenated line breaks\n",
    "    link_pattern = re.compile(r'https?://\\S+|www\\.\\S+')  # remove all links\n",
    "\n",
    "    # Clean each sentence\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        cleaned = ' '.join(sentence.split())  # normalize whitespace\n",
    "        cleaned = link_pattern.sub('', cleaned)  # remove URLs\n",
    "        #cleaned = hyphen_linebreak_pattern.sub(r'\\1\\2', cleaned)  # join hyphenated words\n",
    "        cleaned = repeated_punct_pattern.sub('', cleaned)  # remove .... or ,,,\n",
    "        cleaned = allowed_pattern.sub('', cleaned)  # remove disallowed characters\n",
    "        cleaned = ' '.join(cleaned.split())  # normalize whitespace again\n",
    "        if len(cleaned) > 25:  # skip empty results\n",
    "            cleaned_sentences.append(cleaned)\n",
    "\n",
    "    # Save as CSV\n",
    "    df = pd.DataFrame({'text': cleaned_sentences})\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"CSV saved as: {output_csv.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d4f60",
   "metadata": {},
   "source": [
    "# Guided Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69eaf813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "\n",
    "def create_guided_topic_model():\n",
    "    \"\"\"\n",
    "    Create a BERTopic model with guided topics for internet governance analysis.\n",
    "    \n",
    "    Returns:\n",
    "        BERTopic: Configured topic model with seed topics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define seed topics based on your research themes\n",
    "    seed_topic_list = [\n",
    "        # Openness & Freedom\n",
    "        [\"open\", \"openness\", \"free\", \"freedom\", \"free flow of information\", \n",
    "         \"access\", \"accessibility\", \"transparency\", \"interoperable\", \"common\", \"sharing\"],\n",
    "        \n",
    "        # Human Rights & Democracy\n",
    "        [\"freedom of expression\", \"privacy\", \"democratic\", \"human rights\", \"civil liberties\"],\n",
    "        \n",
    "        # Governance & Cooperation\n",
    "        [\"international cooperation\", \"global governance\", \"shared responsibility\", \n",
    "         \"inclusive\", \"collaboration\"],\n",
    "        \n",
    "        # Innovation & Economy\n",
    "        [\"competition\", \"innovation\", \"entrepreneurship\", \"investment\", \"economic development\"],\n",
    "        \n",
    "        # Regulatory Norms\n",
    "        [\"net neutrality\", \"no censorship\", \"device neutrality\", \"rule of law\"],\n",
    "        \n",
    "        # Sovereignty & Control\n",
    "        [\"sovereignty\", \"cyberspace sovereignty\", \"national territory\", \n",
    "         \"territorial jurisdiction\", \"control\", \"manage\", \"govern\"],\n",
    "        \n",
    "        # Security & Stability\n",
    "        [\"national security\", \"cybersecurity\", \"information security\", \n",
    "         \"public order\", \"regime stability\", \"cyber threats\"],\n",
    "        \n",
    "        # Legal & Regulatory Power\n",
    "        [\"formulate laws\", \"legal measures\", \"constitutional authority\", \n",
    "         \"censorship\", \"information management\"],\n",
    "        \n",
    "        # Protectionism & Defense\n",
    "        [\"safeguard\", \"protect\", \"defend\", \"uphold\", \"counter threats\", \"prevent subversion\"],\n",
    "        \n",
    "        # Ideological & Nationalistic Framing\n",
    "        [\"foreign interference\", \"ideological security\", \"online subversion\", \n",
    "         \"cultural values\", \"strategic stability\"],\n",
    "        \n",
    "        # Exclusive Framing\n",
    "        [\"within our borders\", \"according to national laws\", \"no foreign interference\"]\n",
    "    ]\n",
    "    \n",
    "    # Create vectorizer with better n-gram range for phrase detection\n",
    "    vectorizer_model = CountVectorizer(\n",
    "        ngram_range=(1, 3),  # Include unigrams, bigrams, and trigrams\n",
    "        stop_words=\"english\",\n",
    "        min_df=2,  # Minimum document frequency\n",
    "        max_df=0.95  # Maximum document frequency\n",
    "    )\n",
    "    \n",
    "    # Create TF-IDF transformer\n",
    "    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "    \n",
    "    # Initialize BERTopic model with guided topics\n",
    "    topic_model = BERTopic(\n",
    "        seed_topic_list=seed_topic_list,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        ctfidf_model=ctfidf_model,\n",
    "        verbose=True,\n",
    "        calculate_probabilities=True\n",
    "    )\n",
    "    \n",
    "    return topic_model\n",
    "\n",
    "def fit_and_analyze_topics(docs, topic_model):\n",
    "    \"\"\"\n",
    "    Fit the topic model and return analysis results.\n",
    "    \n",
    "    Args:\n",
    "        docs (list): List of documents to analyze\n",
    "        topic_model (BERTopic): Configured topic model\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (topics, probabilities, topic_model)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Fitting topic model...\")\n",
    "    topics, probs = topic_model.fit_transform(docs)\n",
    "    \n",
    "    print(f\"Number of topics found: {len(set(topics))}\")\n",
    "    print(f\"Number of documents: {len(docs)}\")\n",
    "    \n",
    "    return topics, probs, topic_model\n",
    "\n",
    "def analyze_results(topic_model, topics, docs):\n",
    "    \"\"\"\n",
    "    Analyze and display topic modeling results.\n",
    "    \n",
    "    Args:\n",
    "        topic_model (BERTopic): Fitted topic model\n",
    "        topics (list): Topic assignments for each document\n",
    "        docs (list): Original documents\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get topic information\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    print(\"\\nTopic Information:\")\n",
    "    print(topic_info)\n",
    "    \n",
    "    # Show top words for each topic\n",
    "    print(\"\\nTop words per topic:\")\n",
    "    for topic_id in topic_info['Topic']:\n",
    "        if topic_id != -1:  # Skip outlier topic\n",
    "            words = topic_model.get_topic(topic_id)\n",
    "            print(f\"\\nTopic {topic_id}:\")\n",
    "            print([word for word, _ in words[:]])\n",
    "    \n",
    "    print('******* Print topics: ')\n",
    "\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    print(topic_info)\n",
    "    topic_names = [\n",
    "        topic_info.loc[topic_info['Topic'] == topic, 'Name'].values[0]\n",
    "        if topic in topic_info['Topic'].values else 'Outlier'\n",
    "        for topic in topics\n",
    "    ]\n",
    "    \n",
    "    # Create document-topic dataframe\n",
    "    doc_topic_df = pd.DataFrame({\n",
    "        'Document': docs,\n",
    "        'Topic': topics,\n",
    "        'Topic_Name': topic_names\n",
    "    })\n",
    "    \n",
    "    # Show topic distribution\n",
    "    topic_counts = doc_topic_df['Topic'].value_counts().sort_index()\n",
    "    print(\"\\nTopic Distribution:\")\n",
    "    print(topic_counts)\n",
    "    \n",
    "    return doc_topic_df\n",
    "\n",
    "def save_results(topic_model, doc_topic_df):\n",
    "    \"\"\"\n",
    "    Save topic modeling results to files.\n",
    "    \n",
    "    Args:\n",
    "        topic_model (BERTopic): Fitted topic model\n",
    "        doc_topic_df (pd.DataFrame): Document-topic assignments\n",
    "        output_path (str): Base path for output files\n",
    "    \"\"\"\n",
    "    output_path = 'guided_topic_modeling'\n",
    "    # Save topic information\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    topic_info.to_csv(f\"{output_path}_topic_info.csv\", index=False)\n",
    "    \n",
    "    # Save document-topic assignments\n",
    "    doc_topic_df.to_csv(f\"{output_path}_doc_topics.csv\", index=False)\n",
    "    \n",
    "    # Save detailed topic words\n",
    "    with open(f\"{output_path}_topic_words.txt\", \"w\") as f:\n",
    "        for topic_id in topic_info['Topic']:\n",
    "            if topic_id != -1:\n",
    "                words = topic_model.get_topic(topic_id)\n",
    "                f.write(f\"Topic {topic_id}:\\n\")\n",
    "                f.write(\", \".join([f\"{word} ({score:.3f})\" for word, score in words]))\n",
    "                f.write(\"\\n\\n\")\n",
    "    \n",
    "    print(f\"Results saved to files with prefix: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ad06892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34caaafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline(docs):\n",
    "    \"\"\"\n",
    "    Main pipeline function to run guided topic modeling.\n",
    "    \n",
    "    Args:\n",
    "        docs (list): List of documents to analyze\n",
    "        save_outputs (bool): Whether to save results to files\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (topic_model, topics, probabilities, doc_topic_df)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create topic model\n",
    "    topic_model = create_guided_topic_model()\n",
    "    \n",
    "    # Fit model and get results\n",
    "    topics, probs, fitted_model = fit_and_analyze_topics(docs, topic_model)\n",
    "    \n",
    "    # Analyze results\n",
    "    doc_topic_df = analyze_results(fitted_model, topics, docs)\n",
    "    \n",
    "    # Save results if requested\n",
    "    save_results(fitted_model, doc_topic_df)\n",
    "\n",
    "    #return fitted_model, topics, probs\n",
    "    return fitted_model, topics, probs, doc_topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37ed2486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 18:13:35,615 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting topic model...\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/2n/zg9tdyh15bq7k_499wgv2jgr0000gn/T/ipykernel_94692/3881583056.py\", line 4, in <module>\n",
      "    topic_model, topics, probabilities, results_df = main_pipeline(docs)\n",
      "                                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/2n/zg9tdyh15bq7k_499wgv2jgr0000gn/T/ipykernel_94692/1942867647.py\", line 17, in main_pipeline\n",
      "    topics, probs, fitted_model = fit_and_analyze_topics(docs, topic_model)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/2n/zg9tdyh15bq7k_499wgv2jgr0000gn/T/ipykernel_94692/3961726287.py\", line 93, in fit_and_analyze_topics\n",
      "    topics, probs = topic_model.fit_transform(docs)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/bertopic/_bertopic.py\", line 453, in fit_transform\n",
      "    self.embedding_model = select_backend(self.embedding_model, language=self.language, verbose=self.verbose)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/bertopic/backend/_utils.py\", line 139, in select_backend\n",
      "    return SentenceTransformerBackend(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/bertopic/backend/_sentencetransformers.py\", line 63, in __init__\n",
      "    self.embedding_model = SentenceTransformer(embedding_model)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py\", line 308, in __init__\n",
      "    model_name_or_path = __MODEL_HUB_ORGANIZATION__ + \"/\" + model_name_or_path\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py\", line 1739, in _load_sbert_model\n",
      "    Args:\n",
      "          \n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py\", line 81, in __init__\n",
      "    model_args = {}\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py\", line 181, in _load_model\n",
      "    if isinstance(config, T5Config):\n",
      "                          ^^^^^^^^^^\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 547, in from_pretrained\n",
      "    config, kwargs = AutoConfig.from_pretrained(\n",
      "                                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 791, in keys\n",
      "    self._modules = {}\n",
      "                   ^^^\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 792, in <listcomp>\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 788, in _load_attr_from_module\n",
      "    self._model_mapping = model_mapping\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 700, in getattribute_from_module\n",
      "    from_pretrained_docstring = FROM_PRETRAINED_TORCH_DOCSTRING\n",
      "    ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 2045, in __getattr__\n",
      "    # with this, we don't only want to be able to import these explicitly, we want to be able to import\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 2075, in _get_module\n",
      "    missing_backends.append(backend)\n",
      "    ^^^^^^^\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 2073, in _get_module\n",
      "    missing_backends.append(backend)\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/transformers/models/cohere2/configuration_cohere2.py\", line 22, in <module>\n",
      "    from ...configuration_utils import PretrainedConfig, layer_type_validation\n",
      "ImportError: cannot import name 'layer_type_validation' from 'transformers.configuration_utils' (/Users/wangcancan/anaconda3/lib/python3.11/site-packages/transformers/configuration_utils.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/Users/wangcancan/anaconda3/lib/python3.11/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "filenames = ['india2013', 'china2016', 'southafrica2015', 'netherland2011']\n",
    "docs = pd.read_csv('southafrica2015.csv')['text'].tolist()\n",
    "# topic_model, topics, probabilities = main_pipeline(docs)\n",
    "topic_model, topics, probabilities, results_df = main_pipeline(docs)\n",
    "\n",
    "# Optional: Generate visualizations\n",
    "# topic_model.visualize_topics()\n",
    "# topic_model.visualize_hierarchy()\n",
    "# topic_model.visualize_barchart()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
